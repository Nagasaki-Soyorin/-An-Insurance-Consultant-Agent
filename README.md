# -An-Insurance-Consultant-Agent
这是一个简易的保险咨询智能体的python实现，包括基于JS的可交互前端页面，基于python的LLM中心调度器、中国保险行业协会爬虫和以北大社会保障研究中心的北大保险时评为外部知识库的RAG。其中Agent依赖本地部署的Deepseek-r1:1.5b模型（这是为了课堂展示的便捷性，你当然可以上那个671B的）
## 开始之前想说的
  说实话一开始没准备写的太多，毕竟周六了PPT还没开始做，现在是2025-11-23凌晨0：53，我在等我的拼好饭(联想那边那家疙瘩汤挺好吃的，尤其是藕盒)。所以当我闲的无聊吧，写写技术文档。之后不吱道多久之后才会看，几个月？几年？
  
  至少希望能给看到这个项目的提供一点帮助


## 一、Agent-负责中心调度

  <如果你是想看MCP协议如何建立Client和Server的关系的已经可以请回了，为了简便起见我采用的是json output+正则切分的形式，没用到MCP>

  说回正题，本文的Agent是基于DeepSeek-r1:1.5b模型的，我的电脑是4年前的笔电，能跑起来就不错了。通过Ollama在本地部署，调用也是用的Ollama的接口。

  关于如何部署Ollama上的本地模型，这个教程还挺好的https://textdata.cn/blog/2024-06-14-how-to-download-large-language-model-with-ollama/   

## 二、Tool-1 : 某保险行业协会爬虫

  <不吱道是哪个协会 我忘了 出事了我第一个投降>

  代码添加了一个缓存机制，如果本次调用时检测到了本地指定路径下存在已爬取的文件，并且文件上次爬取时间是今天之内，那么判定命中缓存，直接返回这份文件的内容，反之启动爬虫依次爬取协会人身险数据库中的产品，直到遇到最近一次爬取的文件中的最近的产品；

  爬虫机制，嗯，其实不难找，初级网站的爬虫本质上都是固定密钥+Timestamp形成的，某保协、网易云、某保局都是这样的，具体程序在这里就不解释了，看源码吧

## 三、Tool-3 : RAG

  先滑跪认错。

  本次增强检索生成依赖的是中国社会保障研究中心提供的北大保险时评截至2025-11-20之前的所有评论文章 切分成为chunks后使用ChromaDB自带的Onnx model实现向量化，但是储存使用的是FAISS——因为之前的项目老板让这么干

  但是嘛 因为貌似切的太长了（`500Token+100Overlap`） 所以检索效果实际上不好 但是也无所谓了，毕竟只是演示。直觉上按段落、句子或者其他切分效果应该会更好一些。

## 四、外卖到了 

  先写到这
