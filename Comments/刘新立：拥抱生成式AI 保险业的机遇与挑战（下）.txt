<p style="text-align: center;">　　2023-08-02</p>
<p>　　助力风险减量</p>
<p>　　提升风险评估</p>
<p>　　保险不仅具有损失补偿的功能，还可通过与个体风险密切相关的精算定价，激励保单持有人主动降低风险。这相当于一种间接的风险减量效果，人工智能的应用还有助于直接的风险减量。例如，目前有保险公司为投保的重载货车配备驾驶辅助设备（ADAS）和驾驶员监测设备（DMS），其中，ADAS利用计算机视觉技术，识别车辆前方及周围的道路因素，针对行驶过程中潜在的碰撞危险场景，及时向驾驶员发出预警；DMS可准确识别驾驶员多种疲劳或分神状态，及时发出预警提醒。这些人工智能技术的应用有助于驾驶员提升安全意识、规范驾驶行为。</p>
<p>　　风险因素的识别与评估是产品定价的关键基础，大型语言模型是自然语言处理中的新算法，是深度学习领域的进步，其令人兴奋的是，它们很“大”，这使其具有巨大的信息存储能力，因此可以做出广泛的预测。保险公司可通过自然语言处理来完善其定价模型，从而能够分析大量数据，并识别那些可能意味着潜在风险的模式。例如，对于健康险这类风险不仅涉及被保险人，还涉及医疗服务的复杂产品，传统精算方法的挑战很大，而生成式AI可以综合分析大量的健康险相关数据，包括历史赔付数据、人口统计和医疗资料等，从中识别出潜在的风险因素，有助于更好地理解不同客户的风险特征，提升风险评估的准确性。</p>
<p>　　面临的挑战</p>
<p>　　生成式AI将为保险业的发展提供强大的动力，可以预见，未来这种赋能将改变保险业的许多方面。然而，我们也需要意识到，生成式AI的应用也会面临风险和挑战。例如，AI算法可能存在欠缺公平性和透明性的问题，容易引发数据隐私和道德问题。因此，保险公司在使用生成式AI时，也需要构建及时有效的风险控制机制，包括严谨的数据管理流程、透明的算法审查机制以及健全的伦理标准。</p>
<p>　　首先，应用生成式AI会带来一定的数据风险。目前帮助保险公司根据其掌握越来越丰富的数据进行处理、分析和决策的关键工具之一是机器学习，保险公司可以通过使用机器学习工具来显著提高效率和生产力。然而，由于生成式AI是经过预先训练的模型，训练工具就需要向其提供数据，这并非易事。一方面，可供使用的数据量不一定能够满足要求，向模型提供商传递敏感数据也会增加信息安全风险。人工智能快速收集和处理大量非结构化数据的能力最大的潜力在于承保，而这也正是保险业对个人数据使用的担忧所在，是政府和监管机构所关注的问题。此前三星公司就曾表示，仅使用ChatGPT半个月左右的时间，就发生了三起信息泄露，并且其中还包括一些核心数据。不过Llama2的发布则有可能凭借免费开源和私有化部署解决这些问题。另一方面，无论是大型语言模型还是任何其他类型的模型，都必须有非常好的数据来训练这些模型，而保险业许多可用的数据都是分类数据和低质量、高噪声的混合数据，包含多个特征子集，这些数据可能不利于模型训练。</p>
<p>　　其次，生成式AI的输出具有一定的不一致性。当我们向大型语言模型提问时，它们可能会给出不同的回答，因为模型可能在试图达成某种目标时调整了不同节点上的权重，从而得出了不同的答案。另据美国《财富》杂志网站7月20日报道，斯坦福大学的一项研究对比了广受关注的聊天机器人ChatGPT在数月内完成4项“差异化”任务——解数学题、回答敏感问题、编写软件代码、视觉推理时的表现，发现这项技术执行某些任务的能力存在巨大波动——即“漂移”，因为他们观察到其在6月执行某些任务的表现比3月的时候变得更糟。研究人员观察了GPT-4解答数学题的能力，发现在3月时GPT-4能够在97.6%的答题时间里正确识别出数字17077为质数，但是仅仅3个月后，其答题的正确率却骤降至极低的2.4%。与此同时，GPT-3.5的表现轨迹则几乎相反，其在3月时回答同一问题的正确率仅为7.4%，而在6月时的回答大多是正确的，答题正确率达86.8%。当研究人员要求这两个版本编写代码和接受视觉推理测试时（即要求该技术预测某个图案中的下一个形象），出现了类似的差异化结果。因此，当我们在对大型语言模式进行调整，以改善其在某些任务中的表现时，这样做实际上可能会有许多意想不到的后果，它们或许会影响这个模型在处理其他任务时的表现。在人工智能模型如何回答问题方面，存在各式各样耐人寻味的相互依赖，它们可能在一定程度上导致我们所观察到的这些每况愈下的现象。</p>
<p>　　再次，生成式AI的应用可能会带来一定合规风险。机器学习模型是一种“黑盒”解决方案，其内部工作原理对最终用户来说是不可见的，这使得一些人工智能模型的输出缺乏透明度和可解释性，如果我们过于信任人工智能则可能会事与愿违，数据和算法都可能造成这种情况。生成式AI就像是一种服务，它非常自信、勤勉地解释了一些它实际上不知道的东西，并且看似很有道理，但它只能和他们接受过培训的数据一样好，人工智能如果从被污染的数据池中获取了信息，它就可能被错误的事实和数字影响，正如人工智能可以用于善，也可以用于恶。当前，算法决策替代人工决策已成为趋势，而算法可能存在偏差，破坏保险的公平性。如何确保算法的透明性和公平性，防止算法歧视，是保险业面临的一个挑战。目前大多数人工智能模型尚缺乏“情商”，存在误解客户信息或者输出偏见结论的风险，这可能导致错误的回复，并导致客户投诉增加，从而引发道德、法律和监管挑战，带来合规风险。因此，人工智能的应用不能完全取代人的经验，尤其是在欺诈和索赔方面，任何在人工智能方面的投资都必须有经验丰富的专业人士的支持。</p>
<p>　　最后，生成式AI带有一定技术伦理争议。前些年，说起“技术伦理的边界”这个话题，人们想到的或许还是基因编辑技术，而这两年，人工智能技术则被推到了风口浪尖。例如，具备一定“独创性”的人工智能生成内容是否属于著作权保护的范围？GPT-4发布时的安全文档就写道：GPT-4表现出一些特别令人担忧的能力，例如制订和实施长期计划的能力，积累权力和资源的现象，以及表现出越来越“代理”的行为。这种“代理”暂时并不意味着自我意识的产生，但它所引发的风险已经足够令人警惕。</p>
<p>　　总的来说，人工智能革命是无法回避的，这不仅是一次技术革新，更是一次深度的业务和管理变革。在享受生成式AI带来的便利和效率提升的同时，我们也必须认识到其中潜藏的风险，通过科学的风险管理，实现保险业健康可持续发展。</p>
<p style="text-align: right;">　　转载自《中国银行保险报》“北大保险评论”栏目第821期，2023年8月2日</p>